# Fast Block Partitioning

ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’åŠ é€Ÿå½±åƒ/è¦–è¨Šç·¨ç¢¼å™¨ä¸­çš„å€å¡Šåˆ†å‰²æ±ºç­–ï¼ˆBlock Partitioning Decisionï¼‰ã€‚

## å°ˆæ¡ˆæ¦‚è¿°

æœ¬å°ˆæ¡ˆæ¨¡æ“¬è¦–è¨Šç·¨ç¢¼å™¨ä¸­çš„ **Quadtree Block Partitioning** æ±ºç­–æµç¨‹ï¼Œå±•ç¤ºå¦‚ä½•é€é ML æ¨¡å‹åŠ é€Ÿå‚³çµ±çš„ RDOï¼ˆRate-Distortion Optimizationï¼‰è¨ˆç®—ã€‚

### æ ¸å¿ƒæ¦‚å¿µ

| æ¨¡å‹ | èªªæ˜ | ç‰¹é» |
|------|------|------|
| **C Model** | å®Œæ•´ RDO è¨ˆç®— | ç²¾ç¢ºä½†è€—æ™‚ |
| **Binary Model** | ML å¿«é€Ÿé æ¸¬ | å¿«é€Ÿä½†å¯èƒ½ä¸æº– |
| **Hybrid Model** | æ ¹æ“š Confidence æ±ºå®šç­–ç•¥ | å¹³è¡¡é€Ÿåº¦èˆ‡æº–ç¢ºåº¦ |

### Hybrid ç­–ç•¥

```
if (ML_Confidence >= Threshold):
    æ¡ç”¨ ML é æ¸¬çµæœ (å¿«é€Ÿè·¯å¾‘)
else:
    å›é€€åˆ° RDO è¨ˆç®— (ä¿è­‰å“è³ª)
```

## å°ˆæ¡ˆæ¶æ§‹

```
Fast Block Partitioning/
â”œâ”€â”€ ground_truth.cpp       # Phase 1: Ground Truth è³‡æ–™ç”Ÿæˆï¼ˆåŸºæ–¼ RDO æ±ºç­–ï¼‰
â”œâ”€â”€ train_and_convert.py   # Phase 2: ML æ¨¡å‹è¨“ç·´ & C++ è½‰è­¯
â”œâ”€â”€ main_hybrid.cpp        # Phase 3: Benchmark & Pareto åˆ†æ
â”œâ”€â”€ run_benchmark.py       # å¤šæ¬¡æ¸¬è©¦ä¸¦è¨ˆç®—å¹³å‡å€¼ï¼ˆ3 æ¬¡ï¼‰
â”œâ”€â”€ ml_model_generated.h   # è‡ªå‹•ç”Ÿæˆçš„ ML æ¨¡å‹ C++ ç¨‹å¼ç¢¼
â”œâ”€â”€ block_data.csv         # è¨“ç·´è³‡æ–™é›†
â”œâ”€â”€ pareto_data.csv        # Pareto Frontier æ•¸æ“šï¼ˆå¹³å‡å€¼ï¼‰
â”œâ”€â”€ docs/å°ˆæ¡ˆèªªæ˜.md       # ä¸­æ–‡æŠ€è¡“æ–‡ä»¶
â””â”€â”€ README.md
```

## å¿«é€Ÿé–‹å§‹

### 1. Phase 1: ç”Ÿæˆ Ground Truth

```bash
g++ -o ground_truth ground_truth.cpp && ./ground_truth
```

ç”¢ç”Ÿ `block_data.csv`ï¼ˆè¨“ç·´è³‡æ–™ï¼‰

### 2. Phase 2: è¨“ç·´ ML æ¨¡å‹

```bash
pip install pandas scikit-learn
python train_and_convert.py
```

ç”¢ç”Ÿ `ml_model_generated.h`ï¼ˆC++ å¯ç”¨çš„ Decision Treeï¼‰

### 3. Phase 3: æ•ˆèƒ½æ¸¬è©¦

```bash
g++ -O2 -std=c++11 -o main_hybrid main_hybrid.cpp && ./main_hybrid
```

## RDO (Rate-Distortion Optimization) å¯¦ä½œ

æœ¬å°ˆæ¡ˆå¯¦ä½œçœŸæ­£çš„ RDO è¨ˆç®—ï¼Œè€Œéæ¨¡æ“¬å»¶é²ï¼š

```cpp
// RD Cost = Distortion + Î» Ã— Rate
struct RDOResult {
    double cost_no_split;   // ä¸åˆ‡åˆ†çš„ RD Cost
    double cost_split;      // åˆ‡åˆ†çš„ RD Cost  
    bool should_split;      // cost_split < cost_no_split?
};
```

- **Distortion**: ä½¿ç”¨ SSD (Sum of Squared Differences) è¨ˆç®—å¤±çœŸ
- **Rate**: ä½¿ç”¨ `log2(variance + 1)` ä¼°è¨ˆç·¨ç¢¼ä½å…ƒæ•¸
- **Lambda (Î»)**: Lagrangian multiplierï¼Œæ§åˆ¶ Rate-Distortion æ¬Šè¡¡

## å¯¦é©—çµæœ

### æ¸¬è©¦åœ–ç‰‡

![Test Image Analysis](dummy_image_analysis.png)

### ä¸åŒ Threshold çš„åˆ†å‰²çµæœè¦–è¦ºåŒ–

![Partition Comparison](partition_comparison.png)

**åœ–ä¾‹ï¼š** ğŸ”´ 64Ã—64 | ğŸ©µ 32Ã—32 | ğŸ”µ 16Ã—16 | ğŸŸ¢ 8Ã—8 | ğŸŸ¡ 4Ã—4

### ML-Only vs C Model vs Hybrid

![ML vs C Comparison](partition_ml_vs_c.png)

### Pareto Frontier Analysis

![Pareto Frontier](pareto_frontier.png)

> **æ¸¬è©¦èªªæ˜**: ä»¥ä¸‹çµæœç‚º **3 æ¬¡ç¨ç«‹åŸ·è¡Œçš„å¹³å‡å€¼**ï¼ŒC Model baseline ç´„ 47.6 Î¼s

| Threshold | Time (Î¼s) | Speedup | Accuracy | RDO Ops |
|-----------|-----------|---------|----------|---------|
| 0.50 | 8.63 | **5.51x** Â± 0.22 | 15.34% | 0 |
| 0.60 | 17.52 | **4.20x** Â± 1.94 | 15.34% | 0 |
| 0.70 | 16.89 | **2.84x** Â± 0.15 | 61.96% | 100 |
| 0.80 | 17.52 | **2.75x** Â± 0.33 | 61.96% | 100 |
| 0.90 | 39.82 | **1.19x** Â± 0.00 | 85.28% | 275 |
| 0.95 | 43.58 | 1.11x Â± 0.10 | 95.09% | 345 |
| 1.00 | 38.47 | 1.24x Â± 0.13 | 95.09% | 345 |
| 1.01 | 56.64 | 0.84x Â± 0.08 | **100.00%** | 461 |

### é—œéµç™¼ç¾

- **ML-Only Mode (Î¸=0.5)**: ç´„ **5.5x åŠ é€Ÿ**ï¼Œä½†æº–ç¢ºåº¦åƒ… 15%
- **Threshold = 1.01**: 100% æº–ç¢ºåº¦ï¼Œä½†æ¯” C Model æ…¢ï¼ˆç´„ **0.84x**ï¼‰ï¼Œå› ç‚º ML é¡å¤–é–‹éŠ·
- **æœ€ä½³å¹³è¡¡é» (Î¸=0.7~0.8)**: ç´„ **2.8x åŠ é€Ÿ** èˆ‡ **62% æº–ç¢ºåº¦**
- **é«˜æº–ç¢ºåº¦é¸é … (Î¸=0.9)**: ç´„ **1.2x åŠ é€Ÿ** èˆ‡ **85% æº–ç¢ºåº¦**

### æ¸¬è©¦æµç¨‹

```bash
# 1. ç·¨è­¯
g++ -O2 -std=c++11 -o main_hybrid main_hybrid.cpp

# 2. å¤šæ¬¡æ¸¬è©¦å–å¹³å‡
python run_benchmark.py   # è‡ªå‹•åŸ·è¡Œ 3 æ¬¡ä¸¦ç”Ÿæˆå¹³å‡çµæœ
```

### çµè«–

> "Based on 3 independent runs, we achieved a **~2.8x speedup** with **62% accuracy** (Î¸=0.7~0.8), or **~1.2x speedup** with **85% accuracy** (Î¸=0.9). When threshold > 1.0, all decisions fall back to RDO with 100% accuracy but ~0.84x slower due to ML overhead. This demonstrates the trade-off decisions made in real-world encoder optimization."

## ML æ¨¡å‹è¨­è¨ˆ

### ç‰¹å¾µ (Features)

| ç‰¹å¾µ | è¨ˆç®—æ–¹å¼ | æ„ç¾© |
|------|----------|------|
| `variance` | Î£(x - Î¼)Â² / n | å€å¡Šè¤‡é›œåº¦ |
| `grad_h` | Î£\|x[i,j] - x[i,j+1]\| / n | æ°´å¹³ç´‹ç† |
| `grad_v` | Î£\|x[i,j] - x[i+1,j]\| / n | å‚ç›´ç´‹ç† |

### è¼¸å‡º (å¸¶ Confidence)

```cpp
struct MLPrediction {
    bool should_split;    // é æ¸¬çµæœ
    double confidence;    // ä¿¡å¿ƒåº¦ (0.5 ~ 1.0)
};
```

### Decision Tree è½‰ C++

Python è¨“ç·´çš„æ¨¡å‹è‡ªå‹•è½‰è­¯ç‚ºç´” if-else çµæ§‹ï¼š
- é›¶é‹è¡Œæ™‚ä¾è³´
- Branch Prediction å‹å¥½
- å¯ç›´æ¥åµŒå…¥ Embedded ç³»çµ±

## æŠ€è¡“äº®é»

1. **çœŸå¯¦ RDO è¨ˆç®—**: ä½¿ç”¨ SSD + Rate Estimationï¼Œéæ¨¡æ“¬å»¶é²
2. **Confidence-based Fallback**: ML ä¸ç¢ºå®šæ™‚è‡ªå‹•å›é€€åˆ°ç²¾ç¢ºè¨ˆç®—
3. **Pareto Frontier åˆ†æ**: é‡åŒ– Speed vs Accuracy æ¬Šè¡¡
4. **Auto Code Generation**: Python â†’ C++ è‡ªå‹•è½‰è­¯

## å¾…æ”¹é€²

- [ ] å¢åŠ è¨“ç·´è³‡æ–™å¤šæ¨£æ€§ï¼ˆçœŸå¯¦åœ–ç‰‡/è¦–è¨Šå¹€ï¼‰
- [ ] å˜—è©¦å…¶ä»– ML æ¨¡å‹ï¼ˆRandom Forestã€XGBoostï¼‰
- [ ] å¯¦ä½œ HEVC/VVC ç·¨ç¢¼å™¨æ•´åˆ
- [ ] åŠ å…¥ PSNR/SSIM å“è³ªæŒ‡æ¨™

## ç’°å¢ƒéœ€æ±‚

- C++ Compiler (æ”¯æ´ C++11)
- Python 3.x
- pandas, scikit-learn
